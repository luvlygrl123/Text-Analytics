library(openNLPmodels.en)
setwd("C:/Users/Serina Brenner/Documents/school/spring 2018/PK/")
mytext <- readtext("o_inaguration.txt")
s <- as.String(mytext$text)
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
person_entity_annotator <- Maxent_Entity_Annotator(kind="person")
location_entity_annotator <- Maxent_Entity_Annotator(kind="location")
organization_entity_annotator <- Maxent_Entity_Annotator(kind="organization")
date_entity_annotator <- Maxent_Entity_Annotator(kind="date")
s[person_entity_annotator(s, a2)]
s[location_entity_annotator(s, a2)]
s[organization_entity_annotator(s, a2)]
s[date_entity_annotator(s, a2)]
sort(table(s[person_entity_annotator(s, a2)]))
library(qdap)
sample_text <- "Chipotle is my favorite fast food chain ever.
The burrito bowl is very good.
The guacamole is a hit or a miss though.
I am just blabbering. Let us see if this gets highlighted.
The highlighting feature is pretty cool"
pos_tagging <- pos(sample_text)
pos_tagging$POStagged$POStagged
plot(preprocessed(pos_by(sample_text)))
sample_text <- "visiting aunts can be a nuisance"
pos_tagging <- pos(sample_text)
pos_tagging$POStagged$POStagged
library(markovchain)
p <- matrix(c(1,0.5,0,0,0,0,0.5,0,0,0.5,0,0,0,0,0.5,1), nrow =4, ncol =4)
dtmcA <- new("markovchain",transitionMatrix=p,
states=c("d0","d25","d50","d75"),
name="MarkovChain Gambler")
#matrix
dtmcA
#entity diagram
plot(dtmcA)
#what happens when you start at $50
initialState<-c(0,0,1,0)
steps<-20
finalState<-initialState*dtmcA^steps
finalState
library(rJava)
library(NLP)
library(openNLP)
library(readtext)
library(openNLPmodels.en)
mytext <- readtext("o_inaguration.txt")
s <- as.String(mytext$text)
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
person_entity_annotator <- Maxent_Entity_Annotator(kind="person")
location_entity_annotator <- Maxent_Entity_Annotator(kind="location")
organization_entity_annotator <- Maxent_Entity_Annotator(kind="organization")
date_entity_annotator <- Maxent_Entity_Annotator(kind="date")
s[person_entity_annotator(s, a2)]
s[location_entity_annotator(s, a2)]
s[organization_entity_annotator(s, a2)]
s[date_entity_annotator(s, a2)]
library(qdap)
sample_text <- "Chipotle is my favorite fast food chain ever.
The burrito bowl is very good.
The guacamole is a hit or a miss though.
I am just blabbering. Let us see if this gets highlighted.
The highlighting feature is pretty cool"
pos_tagging <- pos(sample_text)
pos_tagging$POStagged$POStagged
plot(preprocessed(pos_by(sample_text)))
sample_text <- "visiting aunts can be a nuisance"
pos_tagging <- pos(sample_text)
pos_tagging$POStagged$POStagged
library(tidyverse)
library(tidytext)
library(topicmodels)
library(tm)
library(topicmodels)
library(dplyr)
data("AssociatedPress")
AssociatedPress
ap_lda <- LDA(AssociatedPress, k = 2, control = list(seed = 1234))
ap_lda
install.packages('reshape2')
library(reshape2)
install.packages("reshape2")
install.packages("reshape2")
tidy_mytext %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 100)
library(tidytext)
tidy_mytext %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 100)
library(wordcloud)
library(tm)
library(ggplot2)
library(SnowballC)
library(data.table)
library(stringr)
library(qdap)
library(tibble)
library(RWeka)
library(lubridate)
library(lexicon)
library(tidytext)
library(gutenbergr)
library(dplyr)
library(radarchart)
library(reshape2)
setwd("C:/Users/Serina Brenner/Documents/GitHub/Text-Analytics")
mysearch <- read.csv(file="search.csv", header=TRUE, sep=",")
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
# Cleaning MySearch
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
# remove any non funky characters in "text" column
mysearch$text <- iconv(mysearch$text, from = "UTF-8", to = "ASCII", sub = "")
# vector source corpus - cleaning and preprocessing
mysearch_corp <- VCorpus(VectorSource(mysearch$text))
clean_corpus <- function(cleaned_corpus){
removeURL <- content_transformer(function(x) gsub("(f|ht)tp(s?)://\\S+", "", x, perl=T)) # create function
cleaned_corpus <- tm_map(cleaned_corpus, removeURL)                                      # use function to remove urls
cleaned_corpus <- tm_map(cleaned_corpus, content_transformer(replace_abbreviation))      # replace abbreviations
cleaned_corpus <- tm_map(cleaned_corpus, content_transformer(tolower))                   # replace capital letters with lowercase
cleaned_corpus <- tm_map(cleaned_corpus, removePunctuation)                              # remove punctuation
cleaned_corpus <- tm_map(cleaned_corpus, removeWords, stopwords("english"))              # remove common stopwords
# custom stopwords
custom_stop_words <- c("pizza","pinneapple", "hawaiian", "rt", "wsj")
cleaned_corpus <- tm_map(cleaned_corpus, removeWords, custom_stop_words)
cleaned_corpus <- tm_map(cleaned_corpus, stripWhitespace)
return(cleaned_corpus)
}
cleaned_mysearch_corp <- clean_corpus(mysearch_corp)
# TDM/ DTM
TDM_mysearch <- TermDocumentMatrix(cleaned_mysearch_corp)
TDM_mysearch_m <- as.matrix(TDM_mysearch)
# Term Frequency
term_frequency <- rowSums(TDM_mysearch_m)
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
# Unigram (one word) wordcloud
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
word_freqs <- data.frame(term = names(term_frequency), num = term_frequency)
wordcloud(word_freqs$term, word_freqs$num,min.freq=5,max.words=200,colors=brewer.pal(9,"Blues"))
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
# Bigram (two words) wordcloud
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
tokenizer2 <- function(x)
NGramTokenizer(x,Weka_control(min=2,max=2)) # two words shown
bigramtdm <- TermDocumentMatrix(cleaned_mysearch_corp,control = list(tokenize=tokenizer2))
bigramtdmmat <- as.matrix(bigramtdm)
# Term Frequency desc
tf2 <- rowSums(bigramtdmmat)
tf2 <- sort(tf2,dec=TRUE)
# word frequency
word_freqs2 <- data.frame(term = names(tf2), num = tf2)
wordcloud(word_freqs2$term, word_freqs2$num,min.freq=5,max.words=200,colors=brewer.pal(9,"Blues"))
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
# Trigram (three words) wordcloud
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
tokenizer3 <- function(x)
NGramTokenizer(x,Weka_control(min=3,max=3)) # three words shown
trigram_tdm <- TermDocumentMatrix(cleaned_mysearch_corp,control = list(tokenize=tokenizer3))
trigram_tdm_m <- as.matrix(trigram_tdm)
# Term Frequency desc
tf3 <- rowSums(trigram_tdm_m)
tf3 <- sort(tf3,dec=TRUE)
# word frequency
word_freqs3 <- data.frame(term = names(tf3), num = tf3)
wordcloud(word_freqs3$term, word_freqs3$num,min.freq=5,max.words=200,colors=brewer.pal(9,"Blues"))
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
# Inverse Document Frequencies
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
# Unigram
tfidf_tdm <- TermDocumentMatrix(cleaned_mysearch_corp,control=list(weighting=weightTfIdf))
tfidf_tdm_m <- as.matrix(tfidf_tdm)
# Term Frequency desc
term_frequency <- rowSums(tfidf_tdm_m)
term_frequency <- sort(term_frequency,dec=TRUE)
# word frequency
word_freqs <- data.frame(term = names(term_frequency), num = term_frequency)
wordcloud(word_freqs$term, word_freqs$num,min.freq=5,max.words=500,colors=brewer.pal(9,"Blues"))
# Bigram
tfidf_tdm2 <- TermDocumentMatrix(cleaned_mysearch_corp,control = list(tokenize=tokenizer2, weighting=weightTfIdf))
tfidf_tdm2_m <- as.matrix(tfidf_tdm2)
# Term Frequency desc
tf2 <- rowSums(tfidf_tdm2_m)
tf2 <- sort(tf2,dec=TRUE)
# word frequency
word_freqs2 <- data.frame(term = names(tf2), num = tf2)
wordcloud(word_freqs$term, word_freqs2$num,min.freq=5,max.words=500,colors=brewer.pal(9,"Blues"))
# Trigram
tfidf_tdm3 <- TermDocumentMatrix(cleaned_mysearch_corp,control = list(tokenize=tokenizer3, weighting=weightTfIdf))
tfidf_tdm3_m <- as.matrix(tfidf_tdm3)
# Term Frequency desc
tf3 <- rowSums(tfidf_tdm3_m)
tf3 <- sort(tf3,dec=TRUE)
# word frequency
word_freqs3 <- data.frame(term = names(tf3), num = tf3)
wordcloud(word_freqs$term, word_freqs3$num,min.freq=5,max.words=500,colors=brewer.pal(9,"Blues"))
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
# Pos Neg Sentiments
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
# dictionaries
qdapDictionaries::positive.words
qdapDictionaries::negative.words
qdapDictionaries::amplification.words
qdapDictionaries::deamplification.words
qdapDictionaries::negation.words
# use bing lexicon to create sentiment graph
tidy_mytext <- tidy(TermDocumentMatrix(cleaned_mysearch_corp))
bing_lex <- get_sentiments("bing")
mytext_bing <- inner_join(tidy_mytext, bing_lex, by = c("term" = "word"))
mytext_bing$sentiment_n <- ifelse(mytext_bing$sentiment=="negative", -1, 1)
mytext_bing$sentiment_score <- mytext_bing$count*mytext_bing$sentiment_n
aggdata <- aggregate(mytext_bing$sentiment_score, list(index = mytext_bing$document), sum)
sapply(aggdata,typeof)
aggdata$index <- as.numeric(aggdata$index)
ggplot(aggdata, aes(index, x)) + geom_point()
ggplot(aggdata, aes(index, x)) + geom_smooth() + theme_bw()+
geom_hline(yintercept = 0, color = "red")+xlab("sentence")+ylab("sentiment")+
ggtitle("Sentiment")
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
# Split into 2 sets (positive and negative)
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
# Comparison Cloud (positive and negative)
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
tidy_mytext %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 100)
library(tidytext)
tidy_mytext %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 100)
get_sentiments()
install.packages('rlang')
install.packages("rlang")
install.packages("rlang")
term_frequency <- rowSums(TDM_mysearch_m)
wordcloud(word_freqs$term, word_freqs$num,min.freq=5,max.words=200,colors=brewer.pal(9,"Blues"))
library(wordcloud)
library(tm)
library(ggplot2)
library(SnowballC)
library(data.table)
library(stringr)
library(qdap)
library(tibble)
library(RWeka)
library(lubridate)
library(lexicon)
library(tidytext)
library(gutenbergr)
library(dplyr)
library(radarchart)
library(reshape2)
library(rlang)
setwd("C:/Users/Serina Brenner/Documents/GitHub/Text-Analytics")
mysearch <- read.csv(file="search.csv", header=TRUE, sep=",")
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
# Cleaning MySearch
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
# remove any non funky characters in "text" column
mysearch$text <- iconv(mysearch$text, from = "UTF-8", to = "ASCII", sub = "")
# vector source corpus - cleaning and preprocessing
mysearch_corp <- VCorpus(VectorSource(mysearch$text))
clean_corpus <- function(cleaned_corpus){
removeURL <- content_transformer(function(x) gsub("(f|ht)tp(s?)://\\S+", "", x, perl=T)) # create function
cleaned_corpus <- tm_map(cleaned_corpus, removeURL)                                      # use function to remove urls
cleaned_corpus <- tm_map(cleaned_corpus, content_transformer(replace_abbreviation))      # replace abbreviations
cleaned_corpus <- tm_map(cleaned_corpus, content_transformer(tolower))                   # replace capital letters with lowercase
cleaned_corpus <- tm_map(cleaned_corpus, removePunctuation)                              # remove punctuation
cleaned_corpus <- tm_map(cleaned_corpus, removeWords, stopwords("english"))              # remove common stopwords
# custom stopwords
custom_stop_words <- c("pizza","pinneapple", "hawaiian", "rt", "wsj")
cleaned_corpus <- tm_map(cleaned_corpus, removeWords, custom_stop_words)
cleaned_corpus <- tm_map(cleaned_corpus, stripWhitespace)
return(cleaned_corpus)
}
cleaned_mysearch_corp <- clean_corpus(mysearch_corp)
# TDM/ DTM
TDM_mysearch <- TermDocumentMatrix(cleaned_mysearch_corp)
TDM_mysearch_m <- as.matrix(TDM_mysearch)
# Term Frequency
term_frequency <- rowSums(TDM_mysearch_m)
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
# Unigram (one word) wordcloud
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
word_freqs <- data.frame(term = names(term_frequency), num = term_frequency)
wordcloud(word_freqs$term, word_freqs$num,min.freq=5,max.words=200,colors=brewer.pal(9,"Blues"))
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
# Bigram (two words) wordcloud
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
tokenizer2 <- function(x)
NGramTokenizer(x,Weka_control(min=2,max=2)) # two words shown
bigramtdm <- TermDocumentMatrix(cleaned_mysearch_corp,control = list(tokenize=tokenizer2))
bigramtdmmat <- as.matrix(bigramtdm)
# Term Frequency desc
tf2 <- rowSums(bigramtdmmat)
tf2 <- sort(tf2,dec=TRUE)
# word frequency
word_freqs2 <- data.frame(term = names(tf2), num = tf2)
wordcloud(word_freqs2$term, word_freqs2$num,min.freq=5,max.words=200,colors=brewer.pal(9,"Blues"))
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
# Trigram (three words) wordcloud
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
tokenizer3 <- function(x)
NGramTokenizer(x,Weka_control(min=3,max=3)) # three words shown
trigram_tdm <- TermDocumentMatrix(cleaned_mysearch_corp,control = list(tokenize=tokenizer3))
trigram_tdm_m <- as.matrix(trigram_tdm)
# Term Frequency desc
tf3 <- rowSums(trigram_tdm_m)
tf3 <- sort(tf3,dec=TRUE)
# word frequency
word_freqs3 <- data.frame(term = names(tf3), num = tf3)
wordcloud(word_freqs3$term, word_freqs3$num,min.freq=5,max.words=200,colors=brewer.pal(9,"Blues"))
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
# Inverse Document Frequencies
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
# Unigram
tfidf_tdm <- TermDocumentMatrix(cleaned_mysearch_corp,control=list(weighting=weightTfIdf))
tfidf_tdm_m <- as.matrix(tfidf_tdm)
# Term Frequency desc
term_frequency <- rowSums(tfidf_tdm_m)
term_frequency <- sort(term_frequency,dec=TRUE)
# word frequency
word_freqs <- data.frame(term = names(term_frequency), num = term_frequency)
wordcloud(word_freqs$term, word_freqs$num,min.freq=5,max.words=500,colors=brewer.pal(9,"Blues"))
# Bigram
tfidf_tdm2 <- TermDocumentMatrix(cleaned_mysearch_corp,control = list(tokenize=tokenizer2, weighting=weightTfIdf))
tfidf_tdm2_m <- as.matrix(tfidf_tdm2)
# Term Frequency desc
tf2 <- rowSums(tfidf_tdm2_m)
tf2 <- sort(tf2,dec=TRUE)
# word frequency
word_freqs2 <- data.frame(term = names(tf2), num = tf2)
wordcloud(word_freqs$term, word_freqs2$num,min.freq=5,max.words=500,colors=brewer.pal(9,"Blues"))
# Trigram
tfidf_tdm3 <- TermDocumentMatrix(cleaned_mysearch_corp,control = list(tokenize=tokenizer3, weighting=weightTfIdf))
tfidf_tdm3_m <- as.matrix(tfidf_tdm3)
# Term Frequency desc
tf3 <- rowSums(tfidf_tdm3_m)
tf3 <- sort(tf3,dec=TRUE)
# word frequency
word_freqs3 <- data.frame(term = names(tf3), num = tf3)
wordcloud(word_freqs$term, word_freqs3$num,min.freq=5,max.words=500,colors=brewer.pal(9,"Blues"))
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
# Pos Neg Sentiments
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
# dictionaries
qdapDictionaries::positive.words
qdapDictionaries::negative.words
qdapDictionaries::amplification.words
qdapDictionaries::deamplification.words
qdapDictionaries::negation.words
# use bing lexicon to create sentiment graph
tidy_mytext <- tidy(TermDocumentMatrix(cleaned_mysearch_corp))
bing_lex <- get_sentiments("bing")
mytext_bing <- inner_join(tidy_mytext, bing_lex, by = c("term" = "word"))
mytext_bing$sentiment_n <- ifelse(mytext_bing$sentiment=="negative", -1, 1)
mytext_bing$sentiment_score <- mytext_bing$count*mytext_bing$sentiment_n
aggdata <- aggregate(mytext_bing$sentiment_score, list(index = mytext_bing$document), sum)
sapply(aggdata,typeof)
aggdata$index <- as.numeric(aggdata$index)
ggplot(aggdata, aes(index, x)) + geom_point()
ggplot(aggdata, aes(index, x)) + geom_smooth() + theme_bw()+
geom_hline(yintercept = 0, color = "red")+xlab("sentence")+ylab("sentiment")+
ggtitle("Sentiment")
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
# Split into 2 sets (positive and negative)
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
# Comparison Cloud (positive and negative)
#-- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o -- o --
get_sentiments()
tidy_mytext %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 100)
tidy_mytext %>%
inner_join(tidytext::get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 100)
install.packages('reshape2')
tidy_mytext %>%
inner_join(tidytext::get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 100)
tidy_mytext %>%
inner_join(tidy_mytext, bing_lex, by = c("term" = "word")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 100)
bing_lex <- get_sentiments("bing")
tidy_mytext %>%
inner_join(tidy_mytext, bing_lex, by = c("term" = "word")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 100)
bing_lex
tidy_mytext %>%
inner_join(tidytext::get_sentiments("bing"), by = c("term" = "word")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 100)
tidy_mytext %>%
inner_join(tidy_mytext, bing_lex, by = c("term" = "word")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 100)
wordcloud(word_freqs2$term, word_freqs2$num,min.freq=5,max.words=500,colors=brewer.pal(9,"Blues"))
wordcloud(word_freqs3$term, word_freqs3$num,min.freq=5,max.words=500,colors=brewer.pal(9,"Blues"))
tidy_mytext %>%
inner_join(tidy_mytext, bing_lex, by = c("term" = "word")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 100)
mytext_bing %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 100)
mytext_bing %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 100)
count(mytext_bing$term, mytext_bing$sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 100)
count(mytext_bing$term, mytext_bing$sentiment, sort = TRUE) %>%
acast(mytext_bing$term ~ mytext_bing$sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 100)
?acast
count(mytext_bing$term, mytext_bing$sentiment, sort = TRUE)
library(plyr)
count(mytext_bing$term, mytext_bing$sentiment, sort = TRUE) %>%
acast(mytext_bing$term ~ mytext_bing$sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 100)
count(mytext_bing$term, mytext_bing$sentiment) %>%
acast(mytext_bing$term ~ mytext_bing$sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 100)
comparison.cloud(tidy_mytext,max.words=40,random.order=FALSE)
cleaned_mysearch_corp
comparison.cloud(term_frequency,max.words=40,random.order=FALSE)
term.matrix <- TermDocumentMatrix(cleaned_mysearch_corp)
comparison.cloud(term.matrix,max.words=40,random.order=FALSE)
term.matrix <- TermDocumentMatrix(cleaned_mysearch_corp)
term.matrix <- as.matrix(term.matrix)
comparison.cloud(term.matrix,max.words=40,random.order=FALSE,colors = c("gray20", "gray80"))
term.matrix <- TermDocumentMatrix(mytext_bing)
term.matrix <- as.matrix(term.matrix)
comparison.cloud(term.matrix,max.words=40,random.order=FALSE,colors = c("gray20", "gray80"))
term.matrix <- TermDocumentMatrix(cleaned_mysearch_corp)
term.matrix <- as.matrix(term.matrix)
comparison.cloud(term.matrix,max.words=40,random.order=FALSE,colors = c("gray20", "gray80"))
positive <- mytext_bing[mytext_bing$sentiment=="positive",]
negative <- mytext_bing[mytext_bing$sentiment=="negative",]
positive
term.matrix
cleaned_mysearch_corp
term.matrix
TermDocumentMatrix(cleaned_mysearch_corp)
sents = levels(factor(mytext_bing$sentiment))
labels <- lapply(sents, function(x) paste(x,format(round((length((tweet_df[mytext_bing$sentiment ==x,])$text)/length(mytext_bing$sentiment)*100),2),nsmall=2),"%"))
labels <- lapply(sents, function(x) paste(x,format(round((length((mytext_bing[mytext_bing$sentiment ==x,])$text)/length(mytext_bing$sentiment)*100),2),nsmall=2),"%"))
mytext_bing
labels <- lapply(sents, function(x) paste(x,format(round((length((mytext_bing[mytext_bing$sentiment ==x,])$term)/length(mytext_bing$sentiment)*100),2),nsmall=2),"%"))
nemo = length(sents)
emo.docs = rep("", nemo)
for (i in 1:nemo)
{
tmp = mytext_bing[mytext_bing$sentiment == sents[i],]$text
emo.docs[i] = paste(tmp,collapse=" ")
}
for (i in 1:nemo)
{
tmp = mytext_bing[mytext_bing$sentiment == sents[i],]$term
emo.docs[i] = paste(tmp,collapse=" ")
}
corpus = Corpus(VectorSource(emo.docs))
tdm = TermDocumentMatrix(corpus)
tdm = as.matrix(tdm)
colnames(tdm) = labels
comparison.cloud(tdm, colors = brewer.pal(nemo, "Dark2"),
scale = c(3,.5), random.order = FALSE, title.size = 1.5)
comparison.cloud(tdm, colors = brewer.pal(nemo, "blues"),
scale = c(3,.5), random.order = FALSE, title.size = 1.5)
