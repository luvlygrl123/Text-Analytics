MA_mon_chf <-mon_i_ch-residuals(mon_i_ch)
points(MA_mon_chf)
points(MA_mon_chf, type="l", col="#ff5566")
points(MA_mon_chf, type="l", col="#ff5566", ity=3)
MA_mon_chf <-mon_i_ch-residuals(mon_i_ch)
points(MA_mon_chf, type="l", col="#ff5566", ity=3)
MA_mon_chf <-mon_i_ch-residuals(MA_mon_if)
MA_mon_chf <-mon_i_ch-residuals(MA_mon_inf)
points(MA_mon_chf, type="l", col="#ff5566", ity=3)
points(MA_mon_chf, type="l", col="#ff5566", lty=3)
ts.plot(mon_i_ch)
MA_mon_chf <-mon_i_ch-residuals(MA_mon_inf)
points(MA_mon_chf, type="l", col="#ff5566", lty=3)
predict(MA_mon_chf,10)
a <- arima.sim(model = list(c(1,0,1),ar = 2.3, ma = 1.4), n = 1000)
b <- arima.sim(model = list(c(1,0,1),ar = 1.4, ma = 2.3), n = 1000)
a <- arima.sim(model = list(c(1,0,1),ar = 1.2, ma = 1.4), n = 1000)
a <- arima.sim(model = list(c(1,0,1),ar = 0.9, ma = 1.4), n = 1000)
b <- arima.sim(model = list(c(1,0,1),ar = 0.9, ma = 2.3), n = 1000)
plot.ts(cbind(a,b))
dataset(rec)
libray(astsa)
plot.ts(rec)
library(astsa)
plot.ts(rec)
library(tseries)
plot.ts(rec)
adf.test(tsData)
adf.test(rec)
acf(rec)
pacf(rec)
fit1 <- sarima(rec, p=1, q=0, d=0)
fit2 <- sarima(rec, p=2, q=0, d=0)
a <- arima.sim(model = list(c(1,0,0),ar=0.8), n = 100)
b <- arima.sim(model = list(c(1,0,0),ar=-0.95), n = 100)
library(astsa)
library(tseries)
data(rec)
plot.ts(rec)
adf.test(rec)
acf2(rec)
fit_1 <- sarima(rec,p=1,q=0,d=0)
fit_2 <- sarima(rec,p=2,q=0,d=0)
fit_3 <- sarima(rec,p=1,q=0,d=1)
fit_4 <- sarima(rec,p=3,q=0,d=0)
fit_2$ttable
sarima.for(rec,n.ahead=10,p=2,q=0,d=0)
library(astsa)
x<- arima.sim(model = list(order = c(2,0,0), ar = c(1.25,-0.75)), n = 200) + 25
library(astsa)
x<- arima.sim(model = list(order = c(2,0,0), ar = c(1.25,-0.75)), n = 200) + 25
plot.ts(x)
library(tseries)
adf.test(x)
library(isdals)
data(bodyfat)
install.packages(isdals)
library(isdals)
data(bodyfat)
install.packages("isdals")
library(isdals)
data(bodyfat)
fit_1 <- sarima(rec,p=1,q=0,d=0)
fit_2 <- sarima(rec,p=2,q=0,d=0)
fit_3 <- sarima(rec,p=1,q=0,d=1)
fit_4 <- sarima(rec,p=3,q=0,d=0)
x<- arima.sim(model = list(order = c(2,0,0), ar = c(1.25,-0.75)), n = 200) + 25
plot.ts(x)
library(tseries)
adf.test(x)
acf2(x)
fit <- sarima(x, p = 2, q = 0, d = 0)
x<- arima.sim(model = list(order = c(2,0,0), ar = c(1.25,-0.75)), n = 200) + 25
plot.ts(x)
x<- arima.sim(model = list(order = c(1,0,0), ar = c(-0.75)), n = 200) + 25
plot.ts(x)
x<- arima.sim(model = list(order = c(1,0,0), ar = c(0.75)), n = 200) + 25
plot.ts(x)
x<- arima.sim(model = list(order = c(0,0,1), ma = c(0.75)), n = 200) + 25
plot.ts(x)
x<- arima.sim(model = list(order = c(0,0,0)), n = 200) + 25
plot.ts(x)
x<- arima.sim(model = list(order = c(0,1,0)), n = 200) + 25
plot.ts(x)
as.ts(bodyfat)
bf <- arima.sim(bodyfat)
bf <- arima.sim(bodyfat, n=20)
bf
plot(bf)
adf.test(bf)
acf(bf)
acf2(bf)
length(bodyfat)
shape(bodyfat)
bodyfat.summary()
dim(bodyfat)
library(forecast)
auto.arima(bf)
read.csv("C:/Users/Serina Brenner/Documents/school/spring 2018/PK/guess_the_series_1.csv")
guess <- read.csv("C:/Users/Serina Brenner/Documents/school/spring 2018/PK/guess_the_series_1.csv")
as.ts(guess)
plot(guess)
acf(guess)
auto.arima(guess)
libaray(astsa)
a <- arima.sim(model = list(order = c(1,1,0),ar=0.4),n=200)
plot.ts(cbind(a,diff(a)))
library(tseries)
adf.test(a)
adf.test(diff(a))
acf2(a)
acf2(diff(a))
sarima.for(a,p=1,d=1,q=0, n/ahead = 20)
sarima.for(a,p=1,d=1,q=0, n.ahead = 20)
pacf(diff(a))
pacf(a)
pacf(diff(a))
data(oil)
summary(oil)
plot.ts(oil)
head(oil)
oil
oil2
oiltrain <- window(oil, c(2002,1), c(2006,52))
oiltest  <- window(oil, c(2007,1), c(2007,52))
plot(oiltrain)
acf(oiltrain)
acf2(oiltrain)
acf2(diff(oiltrain))
auto.arima(oiltrain)
auto.arima(diff(oiltrain))
sarima.for(oiltrain, p=1, d=1, q=1, n.ahead= 52)
lines(oiltest)
data(globtemp)
gttrain <- window(globtemp, c(1880,1), c(2010,52))
plot(gttrain)
acf2(diff(gttrain))
auto.arima(gttrain)
sarima.for(gttrain, p=1,d=1,q=1, n.ahead=52)
lines(gttest)
gttest  <- window(globtemp, c(2011,1), c(2015,52))
lines(gttest)
sarima.for(gttrain, p=1,d=1,q=1, n.ahead=52)
lines(gttest)
lines(gttest)
sarima.for(oiltrain, p=1, d=1, q=1, n.ahead= 52)
lines(oiltest)
sarima.for(gttrain, p=1,d=1,q=1, n.ahead=52)
lines(gttest)
plot(gttest)
lines(gttest)
sarima.for(gttrain, p=1,d=1,q=1, n.ahead=52)
lines(gttest)
auto.arima(gttrain)
sarima.for(gttrain, p=1,d=1,q=1, n.ahead=52)
lines(gttest)
sarima.for(gttrain, p=1,d=1,q=1, n.ahead=15)
lines(gttest)
sarima.for(gttrain, p=1,d=1,q=1, n.ahead=10)
lines(gttest)
sarima.for(gttrain, p=1,d=1,q=1, n.ahead=5)
lines(gttest)
acf2(diff(gttrain))
gttrain <- window(globtemp, c(1880,1), c(2010,1))
gttest  <- window(globtemp, c(2011,1), c(2015,1))
acf2(diff(gttrain))
auto.arima(gttrain)
sarima.for(gttrain, p=1,d=1,q=1, n.ahead=5)
lines(gttest)
gttest  <- window(globtemp, c(2010,1), c(2015,1))
lines(gttest)
library(tidyr)
df <- read.csv(file="C:/Users/Serina Brenner/Documents/school/spring 2018/visualization/students/int.csv" header=TRUE, set=",")
df <- read.csv(file="C:/Users/Serina Brenner/Documents/school/spring 2018/visualization/students/int.csv", header=TRUE, set=",")
df <- read.csv(file="C:/Users/Serina Brenner/Documents/school/spring 2018/visualization/students/int.csv", header=TRUE)
df
df.head<()
df.head()
head(df)
data_long <- gather(df, Program, Percent, 'Business/ Mgmt.':'Undeclared', factor_key=TRUE)
data_long
df[2]
data_long <- gather(df, Program, Percent, df[2]:df[13], factor_key=TRUE)
df <- read.csv(file="C:/Users/Serina Brenner/Documents/school/spring 2018/visualization/students/int.csv", header=TRUE)
head(df)
data_long <- gather(df, Program, Percent, 'Business Mgmt.':'Undeclared', factor_key=TRUE)
df[2]
data_long
head(df)
data_long <- gather(df, Program, Percent, Business Mgmt.:Undeclared, factor_key=TRUE)
?gather
?reshape2
library(reshape2)
?cast
library(astsa)
wn_2 <- arima.sim(model = list(order = c(0,0,0)), n = 100, mean = 10, sd = 3)
ts.plot (wn_2, col="#aa0000")
wn <- arima.sim(model = list(order = c(0,0,0)), n = 100, mean = 10, sd = 3)
ts.plot (wn, col="#aa0000")
rw <- arima.sim(model = list(order = c(0,1,0)), n = 100)
ts.plot(rw)
ts.plot(rw, col="#aa0000")
library(astsa)
data(jj)
adf.test(jj)
plot.ts(jj)
adf.test(jj)
test(jj)
library(tseries)
adf.test(jj)
acf2(jj)
fit_1 <- sarima(jj,p=1,q=0,d=0)
fit_2 <- sarima(jj,p=2,q=0,d=0)
fit_3 <- sarima(jj,p=1,q=0,d=1)
fit_4 <- sarima(jj,p=3,q=0,d=0)
fit_1 <- sarima(jj,p=1,q=0,d=0)
data(jj)
plot.ts(jj)
adf.test(jj)
library(forecast)
auto.arima(jj)
fit_5 <- sarima(jj,p=0,q=1,d=0)
fit_1 <- sarima(jj,p=1,q=0,d=0)
data(jj)
fit_1
fit_2
fit_3
fit_4
fit_5
fit_1
fit_2
fit_3
fit_4
fit_5
plot.ts(jj)
sarima.for(jj,n.ahead=8,p=1,q=0,d=1)
sarima.for(jj,n.ahead=8,p=0,q=1,d=0)
sarima.for(jj,n.ahead=8,p=1,q=0,d=1)
data(gnp)
plot.ts(gnp)
adf.test(gnp)
acf2(gnp)
auto.arima(gnp)
fit_1 <- sarima(jj,p=1,q=0,d=0)   #AIC : 1.73   BIC : 0.79    pvalar1 : 0.00    pvalxmean : 0.04
fit_2 <- sarima(jj,p=2,q=0,d=0)   #AIC : 1.39   BIC : 0.48    pvalar2 : 0.58    pvalxmean : 0.19
fit_3 <- sarima(jj,p=1,q=0,d=1)   #AIC : 1.32   BIC : 0.38    pvalar1 : 0.00    pvalxmean : 0.05
fit_4 <- sarima(jj,p=3,q=0,d=0)   #AIC : 5.51   BIC : 4.54    pvalar3 : 0.37    pvalxmean : 0.00
fit_5 <- sarima(jj,p=2,q=2,d=1)   #AIC : 3.21   BIC : 2.27    pvalar1 : 0.00    pvalxmean : 0.00
fit_1
fit_1 <- sarima(gnp,p=1,q=0,d=0)   #AIC : 1.73   BIC : 0.79    pvalar1 : 0.00    pvalxmean : 0.04
fit_2 <- sarima(gnp,p=2,q=0,d=0)   #AIC : 1.39   BIC : 0.48    pvalar2 : 0.58    pvalxmean : 0.19
fit_3 <- sarima(gnp,p=1,q=0,d=1)   #AIC : 1.32   BIC : 0.38    pvalar1 : 0.00    pvalxmean : 0.05
fit_4 <- sarima(gnp,p=3,q=0,d=0)   #AIC : 5.51   BIC : 4.54    pvalar3 : 0.37    pvalxmean : 0.00
fit_5 <- sarima(gnp,p=2,q=2,d=1)   #AIC : 3.21   BIC : 2.27    pvalar1 : 0.00    pvalxmean : 0.00
fit_1
fit1 <- sarima(gnp,p=1,q=0,d=0)   #AIC : 1.73   BIC : 0.79    pvalar1 : 0.00    pvalxmean : 0.04
fit2 <- sarima(gnp,p=2,q=0,d=0)   #AIC : 1.39   BIC : 0.48    pvalar2 : 0.58    pvalxmean : 0.19
fit3 <- sarima(gnp,p=1,q=0,d=1)   #AIC : 1.32   BIC : 0.38    pvalar1 : 0.00    pvalxmean : 0.05
fit4 <- sarima(gnp,p=3,q=0,d=0)   #AIC : 5.51   BIC : 4.54    pvalar3 : 0.37    pvalxmean : 0.00
fit5 <- sarima(gnp,p=2,q=2,d=1)   #AIC : 3.21   BIC : 2.27    pvalar1 : 0.00    pvalxmean : 0.00
fit1
fit1 <- sarima(gnp,p=1,q=0,d=0)   #AIC : 1.73   BIC : 0.79    pvalar1 : 0.00    pvalxmean : 0.04
fit1
data(marathon)
data(rec)
plot.ts(rec)
plot.ts(log(rec,4))
plot.ts(rec)
plot.ts(diff(rec))
plot.ts(log(rec,12))
plot.ts(rec)
library(fma)
install.packages('fma')
library(fma)
data(advert)
advert
summary(advert)
ts.plot(advert)
ts.plot(advert, col="aabb33")
ts.plot(advert)
ts.plot(advert, col="#aabb33")
ts.plot(advert, col="#44bb33")
library(fma)
data(advert)
advert
summary(advert)
ts.plot(advert, col="#44bb33")
acf2(advert)
library(astsa)
library(tseries)
acf2(advert)
data(chicken)
ts.plot(chicken)
holt(chicken, exponential = TRUE)
x1 = holt(chicken, exponential = TRUE)
ts.plot(x1)
data(fma::chicken)
data(fma:chicken)
install.packages('fpp2')
library(fpp2)
data(a10)
ts.plot(a10)
x2 = holt(chicken, exponential = TRUE, seasonal = "additive")
ts.plot(x2)
ts.plot(x2)x2
x2
install.packages('Rserve')
library(Rserve)
Rserve()
##########Comparing Lexicons############
library(tm)
library(ggplot2)
library(tidytext)
library(gutenbergr)
library(dplyr)
library(tidyr)
clean_corpus <- function(cleaned_corpus){
cleaned_corpus <- tm_map(cleaned_corpus, removeWords, stopwords("english"))
cleaned_corpus <- tm_map(cleaned_corpus, stripWhitespace)
return(cleaned_corpus)
}
library(tm)
library(qdap)
library(tibble)
library(ggplot2)
library(RWeka)
library(wordcloud)
library(lubridate)
library(lexicon)
library(tidytext)
library(lubridate)
library(gutenbergr)
library(stringr)
library(dplyr)
library(radarchart)
bing_lex <- get_sentiments("bing")
bing_lex
gutenberg_metadata
#-------------------------------------------------------------------------
# Install Packages
#-------------------------------------------------------------------------
library(wordcloud)
library(tm)
library(ggplot2)
library(SnowballC)
library(data.table)
library(stringr)
library(qdap)
library(tibble)
library(RWeka)
library(lubridate)
library(lexicon)
library(tidytext)
library(gutenbergr)
library(dplyr)
library(radarchart)
setwd("C:/Users/Serina Brenner/Documents/GitHub/Text-Analytics")
tweet <- read.csv(file="tableau_text.csv", header=TRUE, sep=",")
tweet$text <- iconv(tweet$text, from = "UTF-8", to = "ASCII", sub = "")
tweet_corp <- VCorpus(VectorSource(tweet$text))
clean_corpus <- function(cleaned_corpus){
removeURL <- content_transformer(function(x) gsub("(f|ht)tp(s?)://\\S+", "", x, perl=T))
cleaned_corpus <- tm_map(cleaned_corpus, removeURL)
cleaned_corpus <- tm_map(cleaned_corpus, content_transformer(replace_abbreviation))
cleaned_corpus <- tm_map(cleaned_corpus, content_transformer(tolower))
cleaned_corpus <- tm_map(cleaned_corpus, removePunctuation)
cleaned_corpus <- tm_map(cleaned_corpus, content_transformer(replace_number))
cleaned_corpus <- tm_map(cleaned_corpus, removeWords, stopwords("english"))
# available stopwords
# stopwords::stopwords()
# Add any custom stop words here
custom_stop_words <- c("")
cleaned_corpus <- tm_map(cleaned_corpus, removeWords, custom_stop_words)
# cleaned_corpus <- tm_map(cleaned_corpus, stemDocument,language = "english")
cleaned_corpus <- tm_map(cleaned_corpus, stripWhitespace)
return(cleaned_corpus)
}
cleaned_tweet_corp <- clean_corpus(tweet_corp)
TDM_Tweets <- TermDocumentMatrix(cleaned_tweet_corp)
TDM_Tweets_m <- as.matrix(TDM_Tweets)
term_frequency <- rowSums(TDM_Tweets_m)
term_frequency
word_freqs <- data.frame(term = names(term_frequency), num = term_frequency)
wordcloud(word_freqs$term, word_freqs$num,min.freq=5,max.words=2000,colors=brewer.pal(10,"BrBG"))
wordcloud(word_freqs$term, word_freqs$num,min.freq=5,max.words=2000,colors=brewer.pal(10,"BrBG"))
tokenizer2 <- function(x)
NGramTokenizer(x,Weka_control(min=2,max=2)) # this is number of words shown together
bigram_tdm <- TermDocumentMatrix(cleaned_tweet_corp,control = list(tokenize=tokenizer2))
bigram_tdm_m <- as.matrix(bigram_tdm)
term_frequency2 <- rowSums(bigram_tdm_m)
term_frequency2 <- sort(term_frequency2,dec=TRUE)
word_freqs2 <- data.frame(term = names(term_frequency), num = term_frequency2)
wordcloud(word_freqs2$term, word_freqs2$num,min.freq=5,max.words=2000,colors=brewer.pal(8, "BrBG"))
word_freqs2 <- data.frame(term = names(term_frequency2), num = term_frequency2)
wordcloud(word_freqs2$term, word_freqs2$num,min.freq=5,max.words=2000,colors=brewer.pal(8, "BrBG"))
tokenizer3 <- function(x)
NGramTokenizer(x,Weka_control(min=3,max=3)) # this is number of words shown together
trigram_tdm <- TermDocumentMatrix(cleaned_tweet_corp,control = list(tokenize=tokenizer3))
trigram_tdm_m <- as.matrix(trigram_tdm)
term_frequency3 <- rowSums(trigram_tdm_m)
term_frequency3 <- sort(term_frequency3,dec=TRUE)
word_freqs3 <- data.frame(term = names(term_frequency3), num = term_frequency3)
wordcloud(word_freqs3$term, word_freqs3$num,min.freq=5,max.words=2000,colors=brewer.pal(8, "BrBG"))
print("------------\nunigram:\n------------\n")
tfidf_tdm <- TermDocumentMatrix(cleaned_tweet_corp,control=list(weighting=weightTfIdf))
tfidf_tdm_m <- as.matrix(tfidf_tdm)
term_frequency <- rowSums(tfidf_tdm_m)
term_frequency <- sort(term_frequency,dec=TRUE)
word_freqs <- data.frame(term = names(term_frequency), num = term_frequency)
wordcloud(word_freqs$term, word_freqs$num,min.freq=5,max.words=2000,colors=brewer.pal(8, "Paired"))
tfidf_tdm2 <- TermDocumentMatrix(cleaned_tweet_corp,control = list(tokenize=tokenizer2, weighting=weightTfIdf))
tfidf_tdm2_m <- as.matrix(tfidf_tdm2)
term_frequency2 <- rowSums(tfidf_tdm2_m)
term_frequency2 <- sort(term_frequency2,dec=TRUE)
word_freqs2 <- data.frame(term = names(term_frequency2), num = term_frequency2)
wordcloud(word_freqs$term, word_freqs2$num,min.freq=5,max.words=2000,colors=brewer.pal(8, "Paired"))
tfidf_tdm3 <- TermDocumentMatrix(cleaned_tweet_corp,control = list(tokenize=tokenizer3, weighting=weightTfIdf))
tfidf_tdm3_m <- as.matrix(tfidf_tdm3)
term_frequency3 <- rowSums(tfidf_tdm3_m)
term_frequency3 <- sort(term_frequency3,dec=TRUE)
word_freqs3 <- data.frame(term = names(term_frequency3), num = term_frequency3)
wordcloud(word_freqs$term, word_freqs3$num,min.freq=5,max.words=2000,colors=brewer.pal(8, "Paired"))
emojis <- data(hash_sentiment_emojis)
nrc_lex <- get_sentiments("nrc")
table(nrc_lex$sentiment)
loughran_lex <- get_sentiments("loughran")
table(loughran_lex$sentiment)
psc <- c()
for(i in 1:nrow(tweets))
{
# Text as Variable
stg <- i.text
# Polarity Table
tbl <- polarity(stg)
# Counts
cnt <- count(tbl)
# Number of Positive Words
pos <- length(cnt$pos.words[[1]])
# Total Number of Words
wds <- cnt$wc
# Verify Polarity Score
pol <- pos / sqrt(wds)
# Add Polarity Score to Dataframe
psc <- rbind(psc, pol)
}
for(i in 1:nrow(tweet))
{
# Text as Variable
stg <- i.text
# Polarity Table
tbl <- polarity(stg)
# Counts
cnt <- count(tbl)
# Number of Positive Words
pos <- length(cnt$pos.words[[1]])
# Total Number of Words
wds <- cnt$wc
# Verify Polarity Score
pol <- pos / sqrt(wds)
# Add Polarity Score to Dataframe
psc <- rbind(psc, pol)
}
for(i in 1:nrow(tweet))
{
# Text as Variable
stg <- i.text
# Polarity Table
tbl <- polarity(stg)
# Counts
cnt <- count(tbl)
# Number of Positive Words
pos <- length(cnt$pos.words[[1]])
# Total Number of Words
wds <- cnt$wc
# Verify Polarity Score
pol <- pos / sqrt(wds)
# Add Polarity Score to Dataframe
psc <- rbind(psc, pol)
}
{
# Text as Variable
stg <- i$text
# Polarity Table
tbl <- polarity(stg)
# Counts
cnt <- count(tbl)
# Number of Positive Words
pos <- length(cnt$pos.words[[1]])
# Total Number of Words
wds <- cnt$wc
# Verify Polarity Score
pol <- pos / sqrt(wds)
# Add Polarity Score to Dataframe
psc <- rbind(psc, pol)
}
tweet$text <- iconv(tweet$text, from = "UTF-8", to = "ASCII", sub = "")
clean_corpus <- function(cleaned_corpus){
cleaned_corpus <- tm_map(cleaned_corpus, removeWords, stopwords("english"))
cleaned_corpus <- tm_map(cleaned_corpus, stripWhitespace)
return(cleaned_corpus)
}
review_corpus <- VCorpus(VectorSource(tweet$text))
cleaned_review_corpus <- clean_corpus(review_corpus)
tidy_mytext <- tidy(TermDocumentMatrix(cleaned_review_corpus))
bing_lex <- get_sentiments("bing")
mytext_bing <- inner_join(tidy_mytext, bing_lex, by = c("term" = "word"))
mytext_bing$sentiment_n <- ifelse(mytext_bing$sentiment=="negative", -1, 1)
mytext_bing$sentiment_score <- mytext_bing$count*mytext_bing$sentiment_n
aggdata <- aggregate(mytext_bing$sentiment_score, list(index = mytext_bing$document), sum)
sapply(aggdata,typeof)
aggdata$index <- as.numeric(aggdata$index)
aggdata$index
